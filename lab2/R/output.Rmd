---
title: "TDAB01 Lab2"
author: "sambl126, johfa688"
date: "3 October 2018"
output: html_document
---


## Uppgift 1

```{r}
set.seed(4711)
x1 <- rgamma(n = 10, shape = 4, scale = 1)
x2 <- rgamma(n = 100, shape = 4, scale = 1)
```

### a)
```{r}
# a)
llgamma <- function(x, alpha, beta)  {
  return(length(x) * (alpha * log(beta) - lgamma(alpha)) +
    (alpha - 1) * sum(log(x)) - beta * sum(x))
}

llgamma(x1, 2, 2)
```

### b)
```{r}
# b)
beta.values = seq(0.01, 10, by=0.01)

v1 = vector()
v2 = vector()
for(beta.v in beta.values) {
  v1 = append(v1, llgamma(x1, 4, beta.v))
  v2 = append(v2, llgamma(x2, 4, beta.v))
}

print(beta.values[which.max(v1)])
print(beta.values[which.max(v2)])
plot(v1, xlab = "Beta index")
plot(v2, xlab = "Beta index")
```

### c)
```{r}
# c)
alpha.values = seq(0.01, 10, by=0.01)

v3 = vector()
v4 = vector()
for(alpha.v in alpha.values) {
  v3 = append(v3, llgamma(x1, alpha.v, 1))
  v4 = append(v4, llgamma(x2, alpha.v, 1))
}
print(alpha.values[which.max(v3)])
print(alpha.values[which.max(v4)])
plot(v3, xlab = "Alpha index")
plot(v4, xlab = "Alpha index")
```

### d)
Framtagandet av funktionen:

L(θ) = P(x1, ..., xn | θ)

ln(L'(θ)) = ∑ (1:n) ln(f(xi|θ))

eftersom det är normalfördelning:

ln(L'(μ, σ²)) = ∑ (1:n) ln(1/sqrt(2πσ²) * exp(- (xi - μ)² / 2σ²))

ln(L'(μ, σ²)) = ln(1/sqrt(2πσ²)) + ∑ (1:n) ln(exp(- (xi - μ)² / 2σ²))

ln(L'(μ, σ²)) = ln(1/sqrt(2πσ²)) + ∑ (1:n) ln(exp(- (xi - μ)² / 2σ²))

ln(L'(μ, σ²)) = -(n/2)ln(2π) - (n/2)ln(σ²) - (1 / 2σ²) ∑ (1:n) (xi - μ)²

```{r}
# d)
llnormal <- function(x, mu, sigma2) {
  - (length(x) / 2) * log(2 * pi) - (length(x) / 2) * log(sigma2) -
    (1 / (2 * sigma2)) * sum((x - mu)^2)
}

llnormal(x = x1, mu = 2, sigma2 = 1)
```



### e)
```{r}
# e)
mu.values = seq(0, 10, by=0.01)

v5 = vector()
v6 = vector()
for(mu.v in mu.values) {
  v5 = append(v5, llnormal(x1, mu.v, 1))
  v6 = append(v6, llnormal(x2, mu.v, 1))
}
mu.values[which.max(v5)]
mu.values[which.max(v6)]
plot(v5, xlab = "Mu index")
plot(v6, xlab = "Mu index")



hist(x1, probability = TRUE, main = "gamma x1", ylim=c(0,0.5))
xfit <- seq(min(x1), max(x1), length.out = length(x1))
yfit <- dgamma(xfit, shape = alpha.values[which.max(v3)], scale = beta.values[which.max(v1)])
lines(xfit, yfit, col="blue", lwd=2)

hist(x2, probability = TRUE, main = "gamma x2", ylim=c(0,0.5))
xfit <- seq(min(x2), max(x2), length.out = length(x2))
yfit <- dgamma(xfit, shape = alpha.values[which.max(v4)], scale = beta.values[which.max(v2)])
lines(xfit, yfit, col="blue", lwd=2)

hist(x1, probability = TRUE, main = "normal x1", ylim=c(0,0.5))
xfit <- seq(min(x1), max(x1), length.out = length(x1))
yfit <- dnorm(xfit, mean = mu.values[which.max(v5)], sd = 1)
lines(xfit, yfit, col="blue", lwd=2)

hist(x2, probability = TRUE, main = "normal x2", ylim=c(0,0.5))
xfit <- seq(min(x2), max(x2), length.out = length(x2))
yfit <- dnorm(xfit, mean = mu.values[which.max(v6)], sd = 1)
lines(xfit, yfit, col="blue", lwd=2)

hist(x1, probability = TRUE, ylim=c(0,0.5))
xfit <- seq(min(x1), max(x1), length.out = length(x1))
yfit <- dgamma(xfit, shape = 4, scale = 1)
lines(xfit, yfit, col="blue", lwd=2)

hist(x2, probability = TRUE, ylim=c(0,0.5))
xfit <- seq(min(x2), max(x2), length.out = length(x2))
yfit <- dgamma(xfit, shape = 4, scale = 1)
lines(xfit, yfit, col="blue", lwd=2)
```
Det vi ser är att gamma-modellerna är betydligt mer precisionssäkra i sin modellering. Detta är väl därför att originalfördelningen är just en gamma-modell.


## Uppgift 2
```{r}
set.seed(4711)
x1 <- rgamma(n = 10, shape = 4, scale = 1)
x2 <- rgamma(n = 100, shape = 4, scale = 1)

gamma_beta_mle <- function(x, alpha) {
  return(length(x) * alpha * 1 / sum(x))
}

print(gamma_beta_mle(x = x1, alpha=4))
print(gamma_beta_mle(x = x2, alpha=4))
```

En slutsats vi kan dra är att skattningen blir mer precisionssäker med fler datapunkter (x1 hade n=10 och x2 hade n=100). Vi vet ju redan att beta är 1 från början.

## Upggift 3

### a)
```{r}
norm_mu_mle <- function(x) {
  return(mean(x))
}

norm_sigma2_mle <- function(x) {
  return((1/length(x)) * sum((x - mean(x))^2))
}

test_x <- 1:10
norm_mu_mle(x = test_x)
norm_sigma2_mle(x = test_x)
```

### b)
```{r}
set.seed(42)
y1 = rnorm(10, mean = 10, sd = 2)
y2 = rnorm(10000, mean = 10, sd = 2)

norm_mu_mle(y1)
norm_mu_mle(y2)
norm_sigma2_mle(y1)
norm_sigma2_mle(y2)
```

10000 dragningar är betydligt närmare de riktiga värdena.


## Uppgift 4

### a)
```{r}
beta1_mle = vector(length = 2000)
beta2_mle = vector(length = 2000)
mu1 = vector(length = 2000)
mu2 = vector(length = 2000)
sigma1 = vector(length = 2000)
sigma2 = vector(length = 2000)
v = 1:2000
for(i in v) {
  x1 <- rgamma(10, shape = 4, scale = 1)
  x2 <- rgamma(10000, shape = 4, scale = 1)
  beta1_mle[i] = gamma_beta_mle(x = x1, alpha = 4)
  beta2_mle[i] = gamma_beta_mle(x = x2, alpha = 4)
  
  y1 <- rnorm(10, mean = 10, sd = 2)
  y2 <- rnorm(10000, mean = 10, sd = 2)
  mu1[i] = norm_mu_mle(x = y1)
  mu2[i] = norm_mu_mle(x = y2)
  sigma1[i] = norm_sigma2_mle(x = y1)
  sigma2[i] = norm_sigma2_mle(x = y2)
}

hist(beta1_mle, col="red")
hist(mu1, col="red")
hist(sigma1, col="red")

hist(beta2_mle, col="blue")
hist(mu2, col="blue")
hist(sigma2, col="blue")
```

Utifrån beta1_mle:
Beta-värdet ska ligga på 1. I diagrammet med 10 utdragningar ligger toppen rätt till men avvikelsen liknar mer av den av en gamma-fördelning än en normalfördelning (lång svans). Diagrammet med 10000 dragningar liknar betydligt mer en normalfördelning. Denna observation finner vi även i de andra parameterskattningarna. Därför blir slutsatsen att fler samplingar leder till en normalfördelning runt det rätta värdet på parametern, d.v.s. uppfyller bättre den Centrala gränsvärdessatsen.
