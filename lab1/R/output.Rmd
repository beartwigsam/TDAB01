---
title: "TDAB01 - Laboration 1"
author: "sambl126, johfa688"
date: "18 September 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/sambl126/TDAB01/TDAB01/lab1/R")
```

## Uppgift 1
### a)
```{r}
v1col = rgb(0, 0, 255, max = 255, alpha = 125, names = "v1-graph")
v2col = rgb(255, 0, 0, max = 255, alpha = 125, names = "v2-graph")

v1.sd <- 4
v1.mean <- 10
v1 <- rnorm(n = 100,   v1.mean, v1.sd)
v2.sd <- 4
v2.mean <- 10
v2 <- rnorm(n = 10000, v2.mean, v2.sd)
hist(v1, probability = TRUE, xlab="Value", col=v1col, main="Histogram of v1 and v2", ylim=c(0, 0.15))
hist(v2, probability = TRUE, xlab="Value", col=v2col, add=TRUE)
legend("topleft", inset=0.02, c("V1", "V2"), fill=c(v1col, v2col), horiz=TRUE, cex=0.8)

# Täthetsfunktion för normalfördelning
v1.pdf <- (1 / sqrt(2 * pi * v1.sd^2)) * exp(-(v1 - v1.mean)^2 / (2 * v1.sd^2))
v2.pdf <- (1 / sqrt(2 * pi * v2.sd^2)) * exp(-(v2 - v2.mean)^2 / (2 * v2.sd^2))
hist(v1.pdf, probability = TRUE, xlab="Probability", 
     col=v1col, main="pdf histogram of v1 and v2", ylim=c(0, 50))
hist(v2.pdf, probability = TRUE, col=v2col, add=TRUE)

legend("topleft", inset=0.02, c("V1", "V2"), fill=c(v1col, v2col), horiz=TRUE, cex=0.8)
```

### b)
Skillnaden är att V2-grafen ser närmare ut som en standard normalfördelning jämfört med V1. Vi ser också i pdf-histogrammet att V2 efterliknar en lenare exponentiell funktion jämfört med V1.

## Uppgift 2
```{r}
x1 = rbinom(n = 10000, size = 1, prob = 0.2)
hist(x1)

x2 = rbinom(10000, 20, 0.1)
hist(x2)

x3 = rbinom(10000, 20, 0.5)
hist(x3)

x4 = rgeom(10000, 0.1)
hist(x4)

x5 = rpois(10000, 10)
hist(x5)

y1 = runif(10000, 0, 1)
hist(y1, breaks = 100)

y2 = rexp(10000, 3)
hist(y2, breaks = 100)

y3 = rgamma(10000, 2, 1)
hist(y3, breaks = 100)

y4 = rt(10000, 3, 0)
hist(y4, breaks = 100)

y5 = rbeta(10000, 0.1, 0.1)
hist(y5, breaks = 100)

y6 = rbeta(10000, 1, 1)
hist(y6, breaks = 100)

y7 = rbeta(10000, 10, 5)
hist(y7, breaks = 100)
```

## Uppgift 3
### a)

```{r}
col1 = rgb(0, 0, 255, max = 255, alpha = 125, names = "v1-graph")
col2 = rgb(255, 0, 0, max = 255, alpha = 125, names = "v2-graph")

x1    = rbinom(n = 1000, size = 10000, prob = 0.001)
x1.dt = dbinom(x = 10000, size = 10000, prob = 0.001)
hist(x1, col = col1, xlim = c(-1,30), breaks = 100)
hist(x1.dt, col = col2, add=TRUE)

x2 = rt(1000, 10000, 0)
hist(x2, col = col2, breaks = 100)

x1.2 = rnorm(1000, mean(x1), sqrt(var(x1)))
hist(x1.2, col = col1, breaks = 100)

```

### b)
#### Binomialfördelning
Binomialfördelningen kan approximeras med en normalfördelning när n är tillräckligt stort.

#### Student-t-fördelning
Student-t-fördelningen närmar sig också en normalfördelning eftersom "svansen" blir mindre med större värde på v.

### c)
Normalfördelningen följer liknande mönster som x1 och x2.


## Uppgift 4

### a)
```{r}
stats = data(faithful)
waiting = faithful[, 2]
hist(waiting, probability = TRUE, breaks = 50, xlim=c(40, 100), ylim=c(0, 0.12))
```

### b)
```{r}
k <- 1:10000
x1 = rbinom(10000, 1, 0.3)
norm1 = rnorm(10000, 15, 3)
norm2 = rnorm(10000, 4, 2)
i = 1
for(x in x1) 
{
  k[i] = sample(x * norm1 + (1 - x) * norm2, 1)
  i = i + 1
}
hist(k, breaks=100)
```

### c)
```{r}
k <- 1:10000
x1 = rbinom(10000, 1, 0.35)
norm1 = rnorm(10000, 54, sqrt(20))
norm2 = rnorm(10000, 79, sqrt(25))
i = 1
for(x in x1) 
{
  k[i] = sample(x * norm1 + (1 - x) * norm2, 1)
  i = i + 1
}
hist(k, breaks=50, probability = TRUE, xlim=c(40, 100), ylim=c(0, 0.12))
```

## Uppgift 5
### a)
```{r}
Y.n = 10
Y.p = 0.1
Y = rbinom(10000, Y.n, Y.p)
Y.d = dbinom(0, Y.n, Y.p)
hist(Y)

n = 0
for(y in Y) 
{
  if(y == 0) {
    n = n + 1
  }
}

print(n)
```

### b)
```{r}
#1
pnorm(0, mean=0, sd=1)
#2
pnorm(1.0, mean=0, sd=1) - pnorm(-1.0, mean=0, sd=1)
#3
1 - pnorm(1.96, mean=0, sd=1)
#4
pbinom(9, 10, 0.1) - pbinom(1, 10, 0.1) # 9 och 1 eftersom det är "mindre än" och "större än" och det är diskreta värden.
#5
pbinom(0, 10, 0.1) - pbinom(-1, 10, 0.1) # -1 för tydlighet
#6
pbinom(10, 10, 0.1) - pbinom(0, 10, 0.1) # Eftersom nu så är det "mindre eller lika med"
```

### c)
```{r}
Y.n = 10
Y.p = 0.1
Y = rbinom(10000, Y.n, Y.p)
X = rnorm(10000, 0, 1)

X.cmf = ecdf(X)
Y.cmf = ecdf(Y)
plot(X.cmf, col="blue")
plot(Y.cmf, col="red", add = TRUE)

#1
X.cmf(0.0)
#2
X.cmf(1.0) - X.cmf(-1.0)
#3
1 - X.cmf(1.96)
#4
Y.cmf(9) - Y.cmf(1) # 9 och 1 eftersom det är "mindre än" och "större än" och det är diskreta värden.
#5
Y.cmf(0)-Y.cmf(-1) # -1 för tydlighet
#6
Y.cmf(10) - Y.cmf(0) # Eftersom nu så är det "mindre eller lika med" etc.
```

## Uppgift 6

```{r}
GS = rbinom(10000, 337, 0.1)
NS.P = runif(10000, 0.02, 0.16)
NS = rbinom(10000, 337, sample(NS.P))

# a)
GS.exp.errors = 0.1 * 337
NS.exp.errors = mean(NS.P) * 337
print(NS.exp.errors)
print(GS.exp.errors)


# b)
NS.cmf = ecdf(NS)
NS.cmf(GS.exp.errors)
plot(NS.cmf, col="blue")

GS.cmf = ecdf(GS)
GS.cmf(GS.exp.errors)
plot(GS.cmf, col="red", add=TRUE)
abline(v=GS.exp.errors, col="grey", lwd=1, lty=2)

# c)
NS.p.50 = 1 - NS.cmf(50)
GS.p.50 = 1 - GS.cmf(50)
print(NS.p.50)
print(GS.p.50)
abline(v=50, col="grey", lwd=1, lty=2)
```

### a)
Nya systemet: 30.43495

Gamla systemet: 33.7

### b)
Det nya systemet ges av det blåa strecket och det gamla systemet ges av det röda. Utifrån grafen så ser vi då att det är upp till ca 60% (0.6) sannolikhet att det nya systemet presterar bättre än det gamla, då 60% 
 av det nya systemets approximerade utfall presterade bättre än det förväntade antalet fel i det gamla systemet.

### c)
Nya systemet: ca 0.096

Gamla systemet: ca 0.0026

Detta vittnar om att det gamla systemet mer sällan presterar riktigt dåligt (enligt antagande), medan
 det nya systemet istället oftare presterar riktigt bra.
 
## Uppgift 7
```{r}
X = runif(100, 0, 1)
Y = runif(100, 0, 1)

villkor <- sqrt(X^2 + Y^2) < 1

# a)
plot(X[villkor], col="red")
plot(Y[villkor], col="blue")

# b)
calcZ <- function(z, n){
  return(4 * (z / n))
}
simXY <- function(b){
  X <- runif(b, 0, 1)
  Y <- runif(b, 0, 1)
  villkor <- sqrt(X^2 + Y^2) < 1
  return(calcZ(length(X[villkor]) + length(Y[villkor]), b))
}

simXY(10000)
simXY(100000)
simXY(1000000)
```

## a)

## b)
Vi tror att den närmar sig 2 * pi.

## c)
Oscillationerna verkar avta med fler samplingar d.v.s. differensen mellan talen blir mindre.

## d)
Integralen ger oss: [x^3 / 3](0 till 2) = 8/3 - 0 = 8/3

## e)
Approximationen ges av: 
```{r}
X <- runif(100000, 0, 2)
Y <- runif(100000, 0, 4)
IHat = mean(Y < X^2)
print(IHat * 2 * 4)
```
Vilket är nära 8/3.

## Uppgift 8

## a)
Binomialfördelning:
```{r}
n = 10
p = 0.2
print(n * p)
```

Gamma-fördelning:
```{r}
alpha = 2
beta = 2
print(alpha/beta)
```

## b)
```{r}
pullXBinom <- function(a.a){
  b = vector()
  for(a in a.a) {
    X = rbinom(a, 10, 0.2)
    X.mean = mean(X)
    b = append(b, X.mean)
  }
  return(b)
}

pullYGamma <- function(a.a){
  b = vector()
  for(a in a.a) {
    Y = rgamma(a, 2, 2)
    Y.mean = mean(Y)
    b = append(b, Y.mean)
  }
  return(b)
}

v = 1:9
totalmean = pullXBinom(v * 10)
totalpull = v * 10
totalmean = append(totalmean, pullXBinom(v * 100))
totalpull = append(totalpull, v * 100)
totalmean = append(totalmean, pullXBinom(v * 1000))
totalpull = append(totalpull, v * 1000)
totalmean = append(totalmean, pullXBinom(10000))
totalpull = append(totalpull, 10000)

plot(totalpull, totalmean, type="l", main="binomial")

totalmean2 = pullYGamma(v * 10)
totalpull2 = v * 10
totalmean2 = append(totalmean2, pullYGamma(v * 100))
totalpull2 = append(totalpull2, v * 100)
totalmean2 = append(totalmean2, pullYGamma(v * 1000))
totalpull2 = append(totalpull2, v * 1000)
totalmean2 = append(totalmean2, pullYGamma(10000))
totalpull2 = append(totalpull2, 10000)

plot(totalpull2, totalmean2, type="l", main="gamma")
```

Som vi ser så närmar sig graferna de värden som beräknades i a) för respektive fördelning.